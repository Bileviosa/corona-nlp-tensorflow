# -*- coding: utf-8 -*-
"""NLP - Corona

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S916HPLAUW7MfmDiH1b7zEDF73GDqSFu
"""

# import package
import nltk, os, re, string

import numpy as np
import pandas as pd 
import time
import numpy
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from bs4 import BeautifulSoup
import string

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import layers, callbacks
from tensorflow.keras import Model, Sequential

nltk.download('wordnet')
nltk.download('stopwords')

# membuka csv file (data uji dan latih)
d_latih = pd.read_csv("drive/MyDrive/Dataset Colabs/Corona_NLP_train.csv", 
                      encoding='latin1')
d_uji = pd.read_csv("drive/MyDrive/Dataset Colabs/Corona_NLP_test.csv", 
                     encoding='latin1')

# membuka data latih
d_latih.head()

# membuka data uji
d_uji.head()

# memeriksa nul pada data latih
d_latih.isnull().sum()

# memeriksa info dari data latih
d_latih.info()

# menghapus kolom yang tidak akan digunakan
d_latih = d_latih.drop(['Location','TweetAt','ScreenName'], axis=1)
d_uji = d_uji.drop(['Location','TweetAt','ScreenName'], axis=1)

d_latih.head()

# fungsi utk mengganti string dengan numerik pada kolom sentimen
# positif = 2, netral = 1, dan negatif = 0
def convert_Sentiment(label):
    if label == "Extremely Positive":
        return 2
    elif label == "Extremely Negative":
        return 0
    elif label == "Positive":
        return 2
    elif label == "Negative":
        return 0
    else:
        return 1

# menerapkan fungsi convert_Sentiment()
d_latih.Sentiment = d_latih.Sentiment.apply(lambda x : convert_Sentiment(x))
d_latih.head()

# fungsi untuk cleaning teks dan menerapkannya pada data latih
def cleaning_text(text):
    stop_words = stopwords.words("english")

    text = re.sub(r'http\S+', " ", text)    # hapus url
    text = re.sub(r'@\w+',' ',text)         # hapus mention
    text = re.sub(r'#\w+', ' ', text)       # hapus hastag
    text = re.sub('r<.*?>',' ', text)       # hapus html tag
    
    # hapus stopwords 
    text = text.split()
    text = " ".join([word for word in text if not word in stop_words])

    for punctuation in string.punctuation:
        text = text.replace(punctuation, "")
    
    return text

# menyimpan hasil cleaning dalam df preprocessing
d_latih['preprocessing_results'] = d_latih['OriginalTweet'].apply(lambda x: cleaning_text(x))

# menampilkan 5 baris hasil preprocessing
for i in range(5):
    print('----------------------------------------------')
    random_number=np.random.randint(0,len(d_latih)-1)
    print(d_latih.preprocessing_results[random_number])
    print('----------------------------------------------\n')

# mencari max kalimat
max_len_words = max(list(d_latih['preprocessing_results'].apply(len)))
print(max_len_words)

# membuat fungsi tokenizer 
def tokenizer(x_train, y_train, max_len_word):
    # karena datanya skewed, maka digunakan "stratify" 
    X_train, X_val, y_train, y_val = train_test_split(x_train, y_train, 
                                                      test_size=.2, shuffle=True, 
                                                      stratify=y_train, random_state=0)

    # Tokenizer
    tokenizer = Tokenizer(num_words=5000)
    tokenizer.fit_on_texts(X_train)
    sequence_dict = tokenizer.word_index
    word_dict = dict((num, val) for (val, num) in sequence_dict.items())

    # Sequence data
    train_sequences = tokenizer.texts_to_sequences(X_train)
    train_padded = pad_sequences(train_sequences,
                                 maxlen=max_len_word,
                                 truncating='post',
                                 padding='post')
    
    val_sequences = tokenizer.texts_to_sequences(X_val)
    val_padded = pad_sequences(val_sequences,
                                maxlen=max_len_word,
                                truncating='post',
                                padding='post', )
    
    print(train_padded.shape)
    print(val_padded.shape)
    print('Total words: {}'.format(len(word_dict)))
    return train_padded, val_padded, y_train, y_val, word_dict

X_train, X_val, y_train, y_val, word_dict = tokenizer(d_latih.preprocessing_results, d_latih.Sentiment, 300)

# jumlah kelas sentimen
num_classes = d_latih.Sentiment.nunique()
print(num_classes)

# mengembangkan model sequential, dengan lstm
model = Sequential([
    layers.Embedding(5000, 300, input_length=300),
    layers.Bidirectional(layers.LSTM(64, return_sequences=True, recurrent_dropout=0.4)),
    #layers.LSTM(64, return_sequences=True, recurrent_dropout=0.4),
    #layers.BatchNormalization(),
    layers.GlobalAveragePooling1D(),   
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.4),
    layers.Dense(num_classes, activation='softmax')
])

model.summary()

# menentukan optimizer dan loss
model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              metrics=['accuracy'])

# class callback
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.90):
      print("\nAkurasi telah mencapai >90%!")
      self.model.stop_training = True
callbacks = myCallback()

start = time.perf_counter()

history = model.fit(X_train, y_train,
                    epochs=10, 
                    validation_data=(X_val, y_val),
                    callbacks=[callbacks], 
                    shuffle=True)

elapsed = time.perf_counter() - start
print('Elapsed %.3f seconds.' % elapsed)

plt.plot(history.history['loss'], color='red')
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='upper right')
plt.show()

plt.plot(history.history['accuracy'], color='blue')
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='lower right')
plt.show()